services:
  ci-runner:
    image: "ghcr.io/matter-labs/zk-environment:cuda-12-latest"
    security_opt:
      - seccomp:unconfined
    command: tail -f /dev/null
    volumes:
      - ..:/usr/src/zksync
      - /usr/src/cache:/usr/src/cache
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - CACHE_DIR=/usr/src/cache
      - SCCACHE_CACHE_SIZE=50g
      - SCCACHE_GCS_BUCKET=matterlabs-infra-sccache-storage
      - SCCACHE_GCS_SERVICE_ACCOUNT=gha-ci-runners@matterlabs-infra.iam.gserviceaccount.com
      - SCCACHE_ERROR_LOG=/tmp/sccache_log.txt
      - SCCACHE_GCS_RW_MODE=READ_WRITE
      - CI=1
      - GITHUB_WORKSPACE=$GITHUB_WORKSPACE
      # We set CUDAARCHS for l4 gpu's
      - CUDAARCHS=89
    # We need to forward all nvidia-devices, as due to bug with cgroups and nvidia-container-runtime (https://github.com/NVIDIA/libnvidia-container/issues/176#issuecomment-1159454366), cgroups are disabled and thou GPU isn't properly forwarded to dind
    devices:
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidiactl:/dev/nvidiactl
      - /dev/nvidia-caps:/dev/nvidia-caps
      - /dev/nvidia-modeset:/dev/nvidia-modeset
      - /dev/nvidia-uvm:/dev/nvidia-uvm
      - /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools
    env_file:
      - ./.env
    extra_hosts:
      - "host:host-gateway"
    profiles:
      - runner
    network_mode: host
    pid: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
